[32mINFO    [0m integration.ha_tests.test_upgrade:test_upgrade.py:123 starting continuous writes to the database
[32mINFO    [0m integration.ha_tests.test_upgrade:test_upgrade.py:127 checking whether writes are increasing
[32mINFO    [0m integration.ha_tests.test_upgrade:test_upgrade.py:130 Get leader unit
[32mINFO    [0m integration.ha_tests.test_upgrade:test_upgrade.py:134 Run pre-upgrade-check action
[32mINFO    [0m integration.ha_tests.test_upgrade:test_upgrade.py:146 Inject dependency fault
[32mINFO    [0m integration.ha_tests.test_upgrade:test_upgrade.py:151 Refresh the charm
[32mINFO    [0m integration.ha_tests.test_upgrade:test_upgrade.py:154 Wait for upgrade to fail
[32mINFO    [0m integration.ha_tests.test_upgrade:test_upgrade.py:161 Ensure continuous_writes while in failure state on remaining units
[32mINFO    [0m integration.ha_tests.test_upgrade:test_upgrade.py:164 Re-run pre-upgrade-check action
[32mINFO    [0m integration.ha_tests.test_upgrade:test_upgrade.py:168 Re-refresh the charm
[32mINFO    [0m integration.ha_tests.test_upgrade:test_upgrade.py:171 Wait for upgrade to start
[32mINFO    [0m integration.ha_tests.test_upgrade:test_upgrade.py:177 Wait for application to recover
[32mINFO    [0m juju.model:model.py:2957 Waiting for model:
  postgresql/0 [idle] blocked: ready to rollback application
  postgresql/1 [idle] waiting: other units upgrading first...
  postgresql/2 [idle] waiting: other units upgrading first...
[32mINFO    [0m juju.model:model.py:2957 Waiting for model:
  postgresql/0 [idle] waiting: other units upgrading first...
  postgresql/1 [executing] maintenance: refreshing the snap
  postgresql/2 [executing] maintenance: updating configuration
[32mINFO    [0m juju.model:model.py:2957 Waiting for model:
  postgresql/0 [idle] waiting: other units upgrading first...
  postgresql/1 [executing] waiting: waiting for database initialisation
  postgresql/2 [executing] waiting: waiting for database initialisation
[32mINFO    [0m juju.model:model.py:2957 Waiting for model:
  postgresql/0 [idle] waiting: other units upgrading first...
  postgresql/1 [executing] waiting: waiting for database initialisation
  postgresql/2 [executing] waiting: waiting for database initialisation
[32mINFO    [0m juju.model:model.py:2957 Waiting for model:
  postgresql/0 [idle] waiting: other units upgrading first...
  postgresql/1 [executing] waiting: waiting for database initialisation
  postgresql/2 [executing] waiting: waiting for database initialisation
[32mINFO    [0m juju.model:model.py:2957 Waiting for model:
  postgresql/0 [idle] waiting: other units upgrading first...
  postgresql/1 [executing] waiting: waiting for database initialisation
  postgresql/2 [executing] waiting: waiting for database initialisation
[32mINFO    [0m juju.model:model.py:2957 Waiting for model:
  postgresql/0 [idle] waiting: other units upgrading first...
  postgresql/1 [executing] waiting: waiting for database initialisation
  postgresql/2 [executing] waiting: waiting for database initialisation
[32mINFO    [0m juju.model:model.py:2957 Waiting for model:
  postgresql/0 [idle] waiting: other units upgrading first...
  postgresql/1 [executing] waiting: waiting for database initialisation
  postgresql/2 [executing] waiting: waiting for database initialisation
[32mINFO    [0m juju.model:model.py:2957 Waiting for model:
  postgresql/0 [idle] waiting: other units upgrading first...
  postgresql/1 [executing] waiting: waiting for database initialisation
  postgresql/2 [executing] waiting: waiting for database initialisation
[32mINFO    [0m juju.model:model.py:2957 Waiting for model:
  postgresql/0 [idle] waiting: other units upgrading first...
  postgresql/1 [executing] waiting: waiting for database initialisation
  postgresql/2 [executing] waiting: waiting for database initialisation
[32mINFO    [0m juju.model:model.py:2957 Waiting for model:
  postgresql/0 [idle] waiting: other units upgrading first...
  postgresql/1 [executing] waiting: waiting for database initialisation
  postgresql/2 [executing] waiting: waiting for database initialisation
[32mINFO    [0m juju.model:model.py:2957 Waiting for model:
  postgresql/0 [idle] waiting: other units upgrading first...
  postgresql/1 [executing] waiting: waiting for database initialisation
  postgresql/2 [executing] waiting: waiting for database initialisation
[32mINFO    [0m juju.model:model.py:2957 Waiting for model:
  postgresql/0 [idle] waiting: other units upgrading first...
  postgresql/1 [executing] waiting: waiting for database initialisation
  postgresql/2 [executing] waiting: waiting for database initialisation
[32mINFO    [0m juju.model:model.py:2957 Waiting for model:
  postgresql/0 [idle] waiting: other units upgrading first...
  postgresql/1 [executing] waiting: waiting for database initialisation
  postgresql/2 [executing] waiting: waiting for database initialisation
[32mINFO    [0m juju.model:model.py:2957 Waiting for model:
  postgresql/0 [executing] waiting: waiting for database initialisation
  postgresql/1 [executing] waiting: waiting for database initialisation
  postgresql/2 [executing] waiting: waiting for database initialisation
[32mINFO    [0m juju.model:model.py:2957 Waiting for model:
  postgresql/0 [executing] active: 
  postgresql/1 [executing] active: 
  postgresql/2 [executing] active: 
[32mINFO    [0m juju.model:model.py:2957 Waiting for model:
  postgresql/0 [idle] active: 
  postgresql/1 [idle] active: 
  postgresql/2 [idle] active: Primary
[32mINFO    [0m integration.ha_tests.test_upgrade:test_upgrade.py:183 Ensure continuous_writes after rollback procedure
[32mINFO    [0m integration.ha_tests.test_upgrade:test_upgrade.py:188 Checking whether no writes were lost
[32mINFO    [0m pytest_operator.plugin:plugin.py:834 Model status:

Model  Controller           Cloud/Region         Version  SLA          Timestamp
test   localhost-localhost  localhost/localhost  3.1.7    unsupported  01:52:33Z

App                  Version  Status  Scale  Charm                Channel      Rev  Exposed  Message
postgresql           14.11    active      3  postgresql                          2  no       
postgresql-test-app           active      1  postgresql-test-app  latest/edge   83  no       received database credentials of the first database

Unit                    Workload  Agent      Machine  Public address  Ports     Message
postgresql-test-app/0*  active    idle       3        10.36.5.60                received database credentials of the first database
postgresql/0*           active    executing  0        10.36.5.29      5432/tcp  
postgresql/1            active    idle       1        10.36.5.122     5432/tcp  
postgresql/2            active    idle       2        10.36.5.153     5432/tcp  Primary

Machine  State    Address      Inst id        Base          AZ  Message
0        started  10.36.5.29   juju-89de03-0  ubuntu@22.04      Running
1        started  10.36.5.122  juju-89de03-1  ubuntu@22.04      Running
2        started  10.36.5.153  juju-89de03-2  ubuntu@22.04      Running
3        started  10.36.5.60   juju-89de03-3  ubuntu@22.04      Running

[32mINFO    [0m pytest_operator.plugin:plugin.py:840 Juju error logs:

machine-0: 01:29:48 ERROR juju.worker.dependency "lxd-container-provisioner" manifold worker returned unexpected error: container types not yet available
machine-0: 01:29:48 ERROR juju.worker.dependency "kvm-container-provisioner" manifold worker returned unexpected error: container types not yet available
machine-0: 01:29:48 ERROR juju.worker.dependency "broker-tracker" manifold worker returned unexpected error: no container types determined
unit-postgresql-0: 01:29:48 ERROR juju.worker.meterstatus error running "meter-status-changed": charm missing from disk
machine-3: 01:29:57 ERROR juju.worker.dependency "kvm-container-provisioner" manifold worker returned unexpected error: container types not yet available
machine-3: 01:29:57 ERROR juju.worker.dependency "broker-tracker" manifold worker returned unexpected error: no container types determined
machine-3: 01:29:57 ERROR juju.worker.dependency "lxd-container-provisioner" manifold worker returned unexpected error: container types not yet available
unit-postgresql-test-app-0: 01:29:57 ERROR juju.worker.meterstatus error running "meter-status-changed": charm missing from disk
machine-2: 01:30:00 ERROR juju.worker.dependency "broker-tracker" manifold worker returned unexpected error: no container types determined
machine-2: 01:30:00 ERROR juju.worker.dependency "kvm-container-provisioner" manifold worker returned unexpected error: container types not yet available
machine-2: 01:30:00 ERROR juju.worker.dependency "lxd-container-provisioner" manifold worker returned unexpected error: container types not yet available
unit-postgresql-2: 01:30:00 ERROR juju.worker.meterstatus error running "meter-status-changed": charm missing from disk
machine-1: 01:30:00 ERROR juju.worker.dependency "kvm-container-provisioner" manifold worker returned unexpected error: container types not yet available
machine-1: 01:30:00 ERROR juju.worker.dependency "lxd-container-provisioner" manifold worker returned unexpected error: container types not yet available
machine-1: 01:30:00 ERROR juju.worker.dependency "broker-tracker" manifold worker returned unexpected error: no container types determined
unit-postgresql-1: 01:30:01 ERROR juju.worker.meterstatus error running "meter-status-changed": charm missing from disk
unit-postgresql-0: 01:32:39 ERROR unit.postgresql/0.juju-log database-peers:1: Failed to list PostgreSQL database users: could not translate host name "None" to address: Temporary failure in name resolution

unit-postgresql-0: 01:43:42 ERROR unit.postgresql/0.juju-log charms.data_platform_libs.v0.upgrade.VersionError - {'message': 'Versions incompatible, snap 14.9 can not be upgraded to 15.0', 'cause': 'Upgrades only supported for specific versions, snap versions satisfying requirement ^15', 'resolution': ''}
unit-postgresql-1: 01:45:56 ERROR unit.postgresql/1.juju-log upgrade:0: replication is not healthy
Traceback (most recent call last):
  File "/var/lib/juju/agents/unit-postgresql-1/charm/src/cluster.py", line 351, in is_replication_healthy
    raise Exception
Exception

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/var/lib/juju/agents/unit-postgresql-1/charm/src/cluster.py", line 340, in is_replication_healthy
    for attempt in Retrying(stop=stop_after_delay(60), wait=wait_fixed(3)):
  File "/var/lib/juju/agents/unit-postgresql-1/charm/venv/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/var/lib/juju/agents/unit-postgresql-1/charm/venv/tenacity/__init__.py", line 326, in iter
    raise retry_exc from fut.exception()
tenacity.RetryError: RetryError[<Future at 0x7fe8a9162740 state=finished raised Exception>]
unit-postgresql-2: 01:45:57 ERROR unit.postgresql/2.juju-log upgrade:0: replication is not healthy
Traceback (most recent call last):
  File "/var/lib/juju/agents/unit-postgresql-2/charm/src/cluster.py", line 351, in is_replication_healthy
    raise Exception
Exception

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/var/lib/juju/agents/unit-postgresql-2/charm/src/cluster.py", line 340, in is_replication_healthy
    for attempt in Retrying(stop=stop_after_delay(60), wait=wait_fixed(3)):
  File "/var/lib/juju/agents/unit-postgresql-2/charm/venv/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/var/lib/juju/agents/unit-postgresql-2/charm/venv/tenacity/__init__.py", line 326, in iter
    raise retry_exc from fut.exception()
tenacity.RetryError: RetryError[<Future at 0x7fcda9097b50 state=finished raised Exception>]
unit-postgresql-1: 01:47:07 ERROR unit.postgresql/1.juju-log upgrade:0: replication is not healthy
Traceback (most recent call last):
  File "/var/lib/juju/agents/unit-postgresql-1/charm/src/cluster.py", line 351, in is_replication_healthy
    raise Exception
Exception

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/var/lib/juju/agents/unit-postgresql-1/charm/src/cluster.py", line 340, in is_replication_healthy
    for attempt in Retrying(stop=stop_after_delay(60), wait=wait_fixed(3)):
  File "/var/lib/juju/agents/unit-postgresql-1/charm/venv/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/var/lib/juju/agents/unit-postgresql-1/charm/venv/tenacity/__init__.py", line 326, in iter
    raise retry_exc from fut.exception()
tenacity.RetryError: RetryError[<Future at 0x7fe8a911ef80 state=finished raised Exception>]
unit-postgresql-2: 01:47:09 ERROR unit.postgresql/2.juju-log upgrade:0: replication is not healthy
Traceback (most recent call last):
  File "/var/lib/juju/agents/unit-postgresql-2/charm/src/cluster.py", line 351, in is_replication_healthy
    raise Exception
Exception

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/var/lib/juju/agents/unit-postgresql-2/charm/src/cluster.py", line 340, in is_replication_healthy
    for attempt in Retrying(stop=stop_after_delay(60), wait=wait_fixed(3)):
  File "/var/lib/juju/agents/unit-postgresql-2/charm/venv/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/var/lib/juju/agents/unit-postgresql-2/charm/venv/tenacity/__init__.py", line 326, in iter
    raise retry_exc from fut.exception()
tenacity.RetryError: RetryError[<Future at 0x7fcda9052cb0 state=finished raised Exception>]
unit-postgresql-1: 01:48:19 ERROR unit.postgresql/1.juju-log upgrade:0: replication is not healthy
Traceback (most recent call last):
  File "/var/lib/juju/agents/unit-postgresql-1/charm/src/cluster.py", line 351, in is_replication_healthy
    raise Exception
Exception

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/var/lib/juju/agents/unit-postgresql-1/charm/src/cluster.py", line 340, in is_replication_healthy
    for attempt in Retrying(stop=stop_after_delay(60), wait=wait_fixed(3)):
  File "/var/lib/juju/agents/unit-postgresql-1/charm/venv/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/var/lib/juju/agents/unit-postgresql-1/charm/venv/tenacity/__init__.py", line 326, in iter
    raise retry_exc from fut.exception()
tenacity.RetryError: RetryError[<Future at 0x7fe8a9162560 state=finished raised Exception>]
unit-postgresql-2: 01:48:21 ERROR unit.postgresql/2.juju-log upgrade:0: replication is not healthy
Traceback (most recent call last):
  File "/var/lib/juju/agents/unit-postgresql-2/charm/src/cluster.py", line 351, in is_replication_healthy
    raise Exception
Exception

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/var/lib/juju/agents/unit-postgresql-2/charm/src/cluster.py", line 340, in is_replication_healthy
    for attempt in Retrying(stop=stop_after_delay(60), wait=wait_fixed(3)):
  File "/var/lib/juju/agents/unit-postgresql-2/charm/venv/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/var/lib/juju/agents/unit-postgresql-2/charm/venv/tenacity/__init__.py", line 326, in iter
    raise retry_exc from fut.exception()
tenacity.RetryError: RetryError[<Future at 0x7fcda9097610 state=finished raised Exception>]
unit-postgresql-1: 01:49:31 ERROR unit.postgresql/1.juju-log upgrade:0: replication is not healthy
Traceback (most recent call last):
  File "/var/lib/juju/agents/unit-postgresql-1/charm/src/cluster.py", line 351, in is_replication_healthy
    raise Exception
Exception

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/var/lib/juju/agents/unit-postgresql-1/charm/src/cluster.py", line 340, in is_replication_healthy
    for attempt in Retrying(stop=stop_after_delay(60), wait=wait_fixed(3)):
  File "/var/lib/juju/agents/unit-postgresql-1/charm/venv/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/var/lib/juju/agents/unit-postgresql-1/charm/venv/tenacity/__init__.py", line 326, in iter
    raise retry_exc from fut.exception()
tenacity.RetryError: RetryError[<Future at 0x7fe8a9163e50 state=finished raised Exception>]
unit-postgresql-2: 01:49:33 ERROR unit.postgresql/2.juju-log upgrade:0: replication is not healthy
Traceback (most recent call last):
  File "/var/lib/juju/agents/unit-postgresql-2/charm/src/cluster.py", line 351, in is_replication_healthy
    raise Exception
Exception

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/var/lib/juju/agents/unit-postgresql-2/charm/src/cluster.py", line 340, in is_replication_healthy
    for attempt in Retrying(stop=stop_after_delay(60), wait=wait_fixed(3)):
  File "/var/lib/juju/agents/unit-postgresql-2/charm/venv/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/var/lib/juju/agents/unit-postgresql-2/charm/venv/tenacity/__init__.py", line 326, in iter
    raise retry_exc from fut.exception()
tenacity.RetryError: RetryError[<Future at 0x7fcda9096da0 state=finished raised Exception>]
unit-postgresql-1: 01:50:42 ERROR unit.postgresql/1.juju-log upgrade:0: replication is not healthy
Traceback (most recent call last):
  File "/var/lib/juju/agents/unit-postgresql-1/charm/src/cluster.py", line 351, in is_replication_healthy
    raise Exception
Exception

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/var/lib/juju/agents/unit-postgresql-1/charm/src/cluster.py", line 340, in is_replication_healthy
    for attempt in Retrying(stop=stop_after_delay(60), wait=wait_fixed(3)):
  File "/var/lib/juju/agents/unit-postgresql-1/charm/venv/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/var/lib/juju/agents/unit-postgresql-1/charm/venv/tenacity/__init__.py", line 326, in iter
    raise retry_exc from fut.exception()
tenacity.RetryError: RetryError[<Future at 0x7fe8a9162e60 state=finished raised Exception>]
unit-postgresql-2: 01:50:45 ERROR unit.postgresql/2.juju-log upgrade:0: replication is not healthy
Traceback (most recent call last):
  File "/var/lib/juju/agents/unit-postgresql-2/charm/src/cluster.py", line 351, in is_replication_healthy
    raise Exception
Exception

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/var/lib/juju/agents/unit-postgresql-2/charm/src/cluster.py", line 340, in is_replication_healthy
    for attempt in Retrying(stop=stop_after_delay(60), wait=wait_fixed(3)):
  File "/var/lib/juju/agents/unit-postgresql-2/charm/venv/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/var/lib/juju/agents/unit-postgresql-2/charm/venv/tenacity/__init__.py", line 326, in iter
    raise retry_exc from fut.exception()
tenacity.RetryError: RetryError[<Future at 0x7fcda9096560 state=finished raised Exception>]

[32mINFO    [0m pytest_operator.plugin:plugin.py:904 Forgetting main...